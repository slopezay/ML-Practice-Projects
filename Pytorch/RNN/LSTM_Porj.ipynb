{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP with LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get the modules needed to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text that is going to be used to train the network is the Old testament of the bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "def read_The_Old_Testament():\n",
    "    with open('./data/Book/the_old_testament.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    return re.sub('[^A-Za-z.,;\\n]+', ' ', text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the old testament \\n\\n genesis\\n\\n in the beginning god created the heaven and the earth. and the earth was without form, and void; and darkness was upon the face of the deep. and the spirit of god moved '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's look at the first 200 characters of the file.\n",
    "text = read_The_Old_Testament()\n",
    "text[:200]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to built out learning model is to tokenize the data (Divide it in characters and and build look up tables to a numeric representation of the characters to feed the network). to do this we define two dictionaries:\n",
    "1. int2char, which maps integers to characters\n",
    "2. char2int, which maps characters to unique integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  6 13 23 18  9 21 23  2 13  8  2 15 14 13 27  2 23  5  5 23 10 13 27\n",
      " 13  8  4  8  5  5 23  4 27 23  2  6 13 23  3 13 10  4 27 27  4 27 10 23\n",
      " 10 18 21 23 26 24 13 15  2 13 21 23  2  6 13 23  6 13 15 28 13 27 23 15\n",
      " 27 21 23  2  6 13 23 13 15 24  2  6 29 23 15 27 21 23  2  6 13 23 13 15\n",
      " 24  2  6 23]\n"
     ]
    }
   ],
   "source": [
    "chars = tuple(set(text)) # set of all unique characters\n",
    "# print(len(chars))\n",
    "int2char = dict(enumerate(chars)) # dictionary mapping integers to characters\n",
    "char2int = {ch: ii for ii, ch in int2char.items()} # dictionary mapping characters to integers\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "print(encoded[:100])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn every encoded character into a one-hot vector that is the expected input to the LSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create mini-batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try: # Slides the window one step forward\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError: # Definition of the end of data case for the targets\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        # put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a instance of the model and trainit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(31, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=31, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 2.9083... Val Loss: 2.8767\n",
      "Epoch: 1/20... Step: 20... Loss: 2.8767... Val Loss: 2.8644\n",
      "Epoch: 1/20... Step: 30... Loss: 2.8750... Val Loss: 2.8617\n",
      "Epoch: 1/20... Step: 40... Loss: 2.8676... Val Loss: 2.8533\n",
      "Epoch: 1/20... Step: 50... Loss: 2.8656... Val Loss: 2.8362\n",
      "Epoch: 1/20... Step: 60... Loss: 2.7984... Val Loss: 2.7642\n",
      "Epoch: 1/20... Step: 70... Loss: 2.6666... Val Loss: 2.6243\n",
      "Epoch: 1/20... Step: 80... Loss: 2.5156... Val Loss: 2.4797\n",
      "Epoch: 1/20... Step: 90... Loss: 2.3441... Val Loss: 2.3122\n",
      "Epoch: 1/20... Step: 100... Loss: 2.2387... Val Loss: 2.2154\n",
      "Epoch: 1/20... Step: 110... Loss: 2.2108... Val Loss: 2.1475\n",
      "Epoch: 1/20... Step: 120... Loss: 2.1531... Val Loss: 2.1044\n",
      "Epoch: 1/20... Step: 130... Loss: 2.0999... Val Loss: 2.0525\n",
      "Epoch: 1/20... Step: 140... Loss: 2.0901... Val Loss: 2.0112\n",
      "Epoch: 1/20... Step: 150... Loss: 2.0261... Val Loss: 1.9720\n",
      "Epoch: 1/20... Step: 160... Loss: 2.0080... Val Loss: 1.9313\n",
      "Epoch: 1/20... Step: 170... Loss: 1.9811... Val Loss: 1.9013\n",
      "Epoch: 1/20... Step: 180... Loss: 1.9354... Val Loss: 1.8629\n",
      "Epoch: 1/20... Step: 190... Loss: 1.9058... Val Loss: 1.8357\n",
      "Epoch: 1/20... Step: 200... Loss: 1.8841... Val Loss: 1.8038\n",
      "Epoch: 1/20... Step: 210... Loss: 1.8699... Val Loss: 1.7744\n",
      "Epoch: 1/20... Step: 220... Loss: 1.8159... Val Loss: 1.7475\n",
      "Epoch: 2/20... Step: 230... Loss: 1.7874... Val Loss: 1.7204\n",
      "Epoch: 2/20... Step: 240... Loss: 1.7624... Val Loss: 1.6945\n",
      "Epoch: 2/20... Step: 250... Loss: 1.7249... Val Loss: 1.6680\n",
      "Epoch: 2/20... Step: 260... Loss: 1.7142... Val Loss: 1.6438\n",
      "Epoch: 2/20... Step: 270... Loss: 1.6976... Val Loss: 1.6178\n",
      "Epoch: 2/20... Step: 280... Loss: 1.6679... Val Loss: 1.5937\n",
      "Epoch: 2/20... Step: 290... Loss: 1.6925... Val Loss: 1.5796\n",
      "Epoch: 2/20... Step: 300... Loss: 1.6506... Val Loss: 1.5605\n",
      "Epoch: 2/20... Step: 310... Loss: 1.6522... Val Loss: 1.5446\n",
      "Epoch: 2/20... Step: 320... Loss: 1.6351... Val Loss: 1.5221\n",
      "Epoch: 2/20... Step: 330... Loss: 1.5942... Val Loss: 1.5068\n",
      "Epoch: 2/20... Step: 340... Loss: 1.5845... Val Loss: 1.4882\n",
      "Epoch: 2/20... Step: 350... Loss: 1.5380... Val Loss: 1.4766\n",
      "Epoch: 2/20... Step: 360... Loss: 1.5483... Val Loss: 1.4586\n",
      "Epoch: 2/20... Step: 370... Loss: 1.5285... Val Loss: 1.4463\n",
      "Epoch: 2/20... Step: 380... Loss: 1.5109... Val Loss: 1.4298\n",
      "Epoch: 2/20... Step: 390... Loss: 1.5330... Val Loss: 1.4185\n",
      "Epoch: 2/20... Step: 400... Loss: 1.5198... Val Loss: 1.4042\n",
      "Epoch: 2/20... Step: 410... Loss: 1.4613... Val Loss: 1.3926\n",
      "Epoch: 2/20... Step: 420... Loss: 1.4629... Val Loss: 1.3839\n",
      "Epoch: 2/20... Step: 430... Loss: 1.4802... Val Loss: 1.3798\n",
      "Epoch: 2/20... Step: 440... Loss: 1.4660... Val Loss: 1.3655\n",
      "Epoch: 3/20... Step: 450... Loss: 1.4524... Val Loss: 1.3562\n",
      "Epoch: 3/20... Step: 460... Loss: 1.4293... Val Loss: 1.3426\n",
      "Epoch: 3/20... Step: 470... Loss: 1.4273... Val Loss: 1.3337\n",
      "Epoch: 3/20... Step: 480... Loss: 1.3828... Val Loss: 1.3232\n",
      "Epoch: 3/20... Step: 490... Loss: 1.4210... Val Loss: 1.3127\n",
      "Epoch: 3/20... Step: 500... Loss: 1.3909... Val Loss: 1.3076\n",
      "Epoch: 3/20... Step: 510... Loss: 1.4093... Val Loss: 1.2997\n",
      "Epoch: 3/20... Step: 520... Loss: 1.4060... Val Loss: 1.2926\n",
      "Epoch: 3/20... Step: 530... Loss: 1.3796... Val Loss: 1.2864\n",
      "Epoch: 3/20... Step: 540... Loss: 1.3899... Val Loss: 1.2779\n",
      "Epoch: 3/20... Step: 550... Loss: 1.3551... Val Loss: 1.2715\n",
      "Epoch: 3/20... Step: 560... Loss: 1.3656... Val Loss: 1.2648\n",
      "Epoch: 3/20... Step: 570... Loss: 1.3480... Val Loss: 1.2533\n",
      "Epoch: 3/20... Step: 580... Loss: 1.3206... Val Loss: 1.2511\n",
      "Epoch: 3/20... Step: 590... Loss: 1.3711... Val Loss: 1.2444\n",
      "Epoch: 3/20... Step: 600... Loss: 1.3553... Val Loss: 1.2417\n",
      "Epoch: 3/20... Step: 610... Loss: 1.3164... Val Loss: 1.2327\n",
      "Epoch: 3/20... Step: 620... Loss: 1.3115... Val Loss: 1.2257\n",
      "Epoch: 3/20... Step: 630... Loss: 1.3062... Val Loss: 1.2252\n",
      "Epoch: 3/20... Step: 640... Loss: 1.2997... Val Loss: 1.2190\n",
      "Epoch: 3/20... Step: 650... Loss: 1.3152... Val Loss: 1.2125\n",
      "Epoch: 3/20... Step: 660... Loss: 1.3158... Val Loss: 1.2117\n",
      "Epoch: 3/20... Step: 670... Loss: 1.3357... Val Loss: 1.2054\n",
      "Epoch: 4/20... Step: 680... Loss: 1.2736... Val Loss: 1.2001\n",
      "Epoch: 4/20... Step: 690... Loss: 1.2653... Val Loss: 1.1953\n",
      "Epoch: 4/20... Step: 700... Loss: 1.2997... Val Loss: 1.1913\n",
      "Epoch: 4/20... Step: 710... Loss: 1.2393... Val Loss: 1.1873\n",
      "Epoch: 4/20... Step: 720... Loss: 1.2691... Val Loss: 1.1848\n",
      "Epoch: 4/20... Step: 730... Loss: 1.3078... Val Loss: 1.1833\n",
      "Epoch: 4/20... Step: 740... Loss: 1.2705... Val Loss: 1.1760\n",
      "Epoch: 4/20... Step: 750... Loss: 1.2412... Val Loss: 1.1744\n",
      "Epoch: 4/20... Step: 760... Loss: 1.2368... Val Loss: 1.1672\n",
      "Epoch: 4/20... Step: 770... Loss: 1.2518... Val Loss: 1.1668\n",
      "Epoch: 4/20... Step: 780... Loss: 1.2517... Val Loss: 1.1622\n",
      "Epoch: 4/20... Step: 790... Loss: 1.2426... Val Loss: 1.1577\n",
      "Epoch: 4/20... Step: 800... Loss: 1.2657... Val Loss: 1.1548\n",
      "Epoch: 4/20... Step: 810... Loss: 1.2659... Val Loss: 1.1542\n",
      "Epoch: 4/20... Step: 820... Loss: 1.2560... Val Loss: 1.1502\n",
      "Epoch: 4/20... Step: 830... Loss: 1.2288... Val Loss: 1.1446\n",
      "Epoch: 4/20... Step: 840... Loss: 1.2111... Val Loss: 1.1423\n",
      "Epoch: 4/20... Step: 850... Loss: 1.2231... Val Loss: 1.1469\n",
      "Epoch: 4/20... Step: 860... Loss: 1.2302... Val Loss: 1.1378\n",
      "Epoch: 4/20... Step: 870... Loss: 1.2088... Val Loss: 1.1349\n",
      "Epoch: 4/20... Step: 880... Loss: 1.2256... Val Loss: 1.1313\n",
      "Epoch: 4/20... Step: 890... Loss: 1.2375... Val Loss: 1.1312\n",
      "Epoch: 5/20... Step: 900... Loss: 1.1952... Val Loss: 1.1291\n",
      "Epoch: 5/20... Step: 910... Loss: 1.2178... Val Loss: 1.1239\n",
      "Epoch: 5/20... Step: 920... Loss: 1.1740... Val Loss: 1.1249\n",
      "Epoch: 5/20... Step: 930... Loss: 1.2007... Val Loss: 1.1212\n",
      "Epoch: 5/20... Step: 940... Loss: 1.1760... Val Loss: 1.1202\n",
      "Epoch: 5/20... Step: 950... Loss: 1.2044... Val Loss: 1.1176\n",
      "Epoch: 5/20... Step: 960... Loss: 1.1742... Val Loss: 1.1149\n",
      "Epoch: 5/20... Step: 970... Loss: 1.1911... Val Loss: 1.1132\n",
      "Epoch: 5/20... Step: 980... Loss: 1.1828... Val Loss: 1.1114\n",
      "Epoch: 5/20... Step: 990... Loss: 1.1880... Val Loss: 1.1064\n",
      "Epoch: 5/20... Step: 1000... Loss: 1.2062... Val Loss: 1.1061\n",
      "Epoch: 5/20... Step: 1010... Loss: 1.1904... Val Loss: 1.1032\n",
      "Epoch: 5/20... Step: 1020... Loss: 1.1675... Val Loss: 1.1001\n",
      "Epoch: 5/20... Step: 1030... Loss: 1.1765... Val Loss: 1.1011\n",
      "Epoch: 5/20... Step: 1040... Loss: 1.1720... Val Loss: 1.0989\n",
      "Epoch: 5/20... Step: 1050... Loss: 1.1867... Val Loss: 1.0983\n",
      "Epoch: 5/20... Step: 1060... Loss: 1.1554... Val Loss: 1.0944\n",
      "Epoch: 5/20... Step: 1070... Loss: 1.1464... Val Loss: 1.0910\n",
      "Epoch: 5/20... Step: 1080... Loss: 1.1814... Val Loss: 1.0912\n",
      "Epoch: 5/20... Step: 1090... Loss: 1.1683... Val Loss: 1.0892\n",
      "Epoch: 5/20... Step: 1100... Loss: 1.1678... Val Loss: 1.0862\n",
      "Epoch: 5/20... Step: 1110... Loss: 1.1695... Val Loss: 1.0878\n",
      "Epoch: 5/20... Step: 1120... Loss: 1.2282... Val Loss: 1.0853\n",
      "Epoch: 6/20... Step: 1130... Loss: 1.1381... Val Loss: 1.0834\n",
      "Epoch: 6/20... Step: 1140... Loss: 1.1263... Val Loss: 1.0833\n",
      "Epoch: 6/20... Step: 1150... Loss: 1.1544... Val Loss: 1.0807\n",
      "Epoch: 6/20... Step: 1160... Loss: 1.1532... Val Loss: 1.0760\n",
      "Epoch: 6/20... Step: 1170... Loss: 1.1827... Val Loss: 1.0778\n",
      "Epoch: 6/20... Step: 1180... Loss: 1.1446... Val Loss: 1.0794\n",
      "Epoch: 6/20... Step: 1190... Loss: 1.1731... Val Loss: 1.0752\n",
      "Epoch: 6/20... Step: 1200... Loss: 1.1260... Val Loss: 1.0733\n",
      "Epoch: 6/20... Step: 1210... Loss: 1.1076... Val Loss: 1.0720\n",
      "Epoch: 6/20... Step: 1220... Loss: 1.1203... Val Loss: 1.0709\n",
      "Epoch: 6/20... Step: 1230... Loss: 1.1561... Val Loss: 1.0673\n",
      "Epoch: 6/20... Step: 1240... Loss: 1.1340... Val Loss: 1.0687\n",
      "Epoch: 6/20... Step: 1250... Loss: 1.1173... Val Loss: 1.0658\n",
      "Epoch: 6/20... Step: 1260... Loss: 1.1806... Val Loss: 1.0661\n",
      "Epoch: 6/20... Step: 1270... Loss: 1.1634... Val Loss: 1.0666\n",
      "Epoch: 6/20... Step: 1280... Loss: 1.1022... Val Loss: 1.0640\n",
      "Epoch: 6/20... Step: 1290... Loss: 1.1242... Val Loss: 1.0647\n",
      "Epoch: 6/20... Step: 1300... Loss: 1.1074... Val Loss: 1.0600\n",
      "Epoch: 6/20... Step: 1310... Loss: 1.1553... Val Loss: 1.0577\n",
      "Epoch: 6/20... Step: 1320... Loss: 1.1125... Val Loss: 1.0567\n",
      "Epoch: 6/20... Step: 1330... Loss: 1.1390... Val Loss: 1.0576\n",
      "Epoch: 6/20... Step: 1340... Loss: 1.1203... Val Loss: 1.0587\n",
      "Epoch: 7/20... Step: 1350... Loss: 1.1031... Val Loss: 1.0560\n",
      "Epoch: 7/20... Step: 1360... Loss: 1.1150... Val Loss: 1.0521\n",
      "Epoch: 7/20... Step: 1370... Loss: 1.0668... Val Loss: 1.0530\n",
      "Epoch: 7/20... Step: 1380... Loss: 1.0926... Val Loss: 1.0532\n",
      "Epoch: 7/20... Step: 1390... Loss: 1.0959... Val Loss: 1.0530\n",
      "Epoch: 7/20... Step: 1400... Loss: 1.0805... Val Loss: 1.0496\n",
      "Epoch: 7/20... Step: 1410... Loss: 1.1370... Val Loss: 1.0512\n",
      "Epoch: 7/20... Step: 1420... Loss: 1.1189... Val Loss: 1.0491\n",
      "Epoch: 7/20... Step: 1430... Loss: 1.1287... Val Loss: 1.0459\n",
      "Epoch: 7/20... Step: 1440... Loss: 1.1198... Val Loss: 1.0477\n",
      "Epoch: 7/20... Step: 1450... Loss: 1.0976... Val Loss: 1.0448\n",
      "Epoch: 7/20... Step: 1460... Loss: 1.1137... Val Loss: 1.0445\n",
      "Epoch: 7/20... Step: 1470... Loss: 1.0750... Val Loss: 1.0433\n",
      "Epoch: 7/20... Step: 1480... Loss: 1.0867... Val Loss: 1.0427\n",
      "Epoch: 7/20... Step: 1490... Loss: 1.1019... Val Loss: 1.0396\n",
      "Epoch: 7/20... Step: 1500... Loss: 1.0633... Val Loss: 1.0412\n",
      "Epoch: 7/20... Step: 1510... Loss: 1.1092... Val Loss: 1.0404\n",
      "Epoch: 7/20... Step: 1520... Loss: 1.0958... Val Loss: 1.0376\n",
      "Epoch: 7/20... Step: 1530... Loss: 1.0730... Val Loss: 1.0367\n",
      "Epoch: 7/20... Step: 1540... Loss: 1.0914... Val Loss: 1.0372\n",
      "Epoch: 7/20... Step: 1550... Loss: 1.1002... Val Loss: 1.0360\n",
      "Epoch: 7/20... Step: 1560... Loss: 1.0839... Val Loss: 1.0350\n",
      "Epoch: 8/20... Step: 1570... Loss: 1.0867... Val Loss: 1.0341\n",
      "Epoch: 8/20... Step: 1580... Loss: 1.0721... Val Loss: 1.0342\n",
      "Epoch: 8/20... Step: 1590... Loss: 1.0891... Val Loss: 1.0348\n",
      "Epoch: 8/20... Step: 1600... Loss: 1.0610... Val Loss: 1.0327\n",
      "Epoch: 8/20... Step: 1610... Loss: 1.0946... Val Loss: 1.0298\n",
      "Epoch: 8/20... Step: 1620... Loss: 1.0609... Val Loss: 1.0319\n",
      "Epoch: 8/20... Step: 1630... Loss: 1.0902... Val Loss: 1.0318\n",
      "Epoch: 8/20... Step: 1640... Loss: 1.0974... Val Loss: 1.0320\n",
      "Epoch: 8/20... Step: 1650... Loss: 1.0873... Val Loss: 1.0310\n",
      "Epoch: 8/20... Step: 1660... Loss: 1.1005... Val Loss: 1.0324\n",
      "Epoch: 8/20... Step: 1670... Loss: 1.0726... Val Loss: 1.0291\n",
      "Epoch: 8/20... Step: 1680... Loss: 1.0900... Val Loss: 1.0250\n",
      "Epoch: 8/20... Step: 1690... Loss: 1.0721... Val Loss: 1.0272\n",
      "Epoch: 8/20... Step: 1700... Loss: 1.0509... Val Loss: 1.0250\n",
      "Epoch: 8/20... Step: 1710... Loss: 1.1114... Val Loss: 1.0232\n",
      "Epoch: 8/20... Step: 1720... Loss: 1.0987... Val Loss: 1.0242\n",
      "Epoch: 8/20... Step: 1730... Loss: 1.0592... Val Loss: 1.0254\n",
      "Epoch: 8/20... Step: 1740... Loss: 1.0438... Val Loss: 1.0229\n",
      "Epoch: 8/20... Step: 1750... Loss: 1.0733... Val Loss: 1.0207\n",
      "Epoch: 8/20... Step: 1760... Loss: 1.0570... Val Loss: 1.0202\n",
      "Epoch: 8/20... Step: 1770... Loss: 1.0865... Val Loss: 1.0182\n",
      "Epoch: 8/20... Step: 1780... Loss: 1.0795... Val Loss: 1.0196\n",
      "Epoch: 8/20... Step: 1790... Loss: 1.1023... Val Loss: 1.0219\n",
      "Epoch: 9/20... Step: 1800... Loss: 1.0500... Val Loss: 1.0200\n",
      "Epoch: 9/20... Step: 1810... Loss: 1.0327... Val Loss: 1.0176\n",
      "Epoch: 9/20... Step: 1820... Loss: 1.0770... Val Loss: 1.0189\n",
      "Epoch: 9/20... Step: 1830... Loss: 1.0316... Val Loss: 1.0174\n",
      "Epoch: 9/20... Step: 1840... Loss: 1.0706... Val Loss: 1.0210\n",
      "Epoch: 9/20... Step: 1850... Loss: 1.1007... Val Loss: 1.0156\n",
      "Epoch: 9/20... Step: 1860... Loss: 1.0671... Val Loss: 1.0168\n",
      "Epoch: 9/20... Step: 1870... Loss: 1.0483... Val Loss: 1.0154\n",
      "Epoch: 9/20... Step: 1880... Loss: 1.0455... Val Loss: 1.0135\n",
      "Epoch: 9/20... Step: 1890... Loss: 1.0588... Val Loss: 1.0159\n",
      "Epoch: 9/20... Step: 1900... Loss: 1.0666... Val Loss: 1.0122\n",
      "Epoch: 9/20... Step: 1910... Loss: 1.0553... Val Loss: 1.0118\n",
      "Epoch: 9/20... Step: 1920... Loss: 1.0754... Val Loss: 1.0133\n",
      "Epoch: 9/20... Step: 1930... Loss: 1.0706... Val Loss: 1.0127\n",
      "Epoch: 9/20... Step: 1940... Loss: 1.0690... Val Loss: 1.0103\n",
      "Epoch: 9/20... Step: 1950... Loss: 1.0483... Val Loss: 1.0123\n",
      "Epoch: 9/20... Step: 1960... Loss: 1.0395... Val Loss: 1.0132\n",
      "Epoch: 9/20... Step: 1970... Loss: 1.0406... Val Loss: 1.0113\n",
      "Epoch: 9/20... Step: 1980... Loss: 1.0541... Val Loss: 1.0087\n",
      "Epoch: 9/20... Step: 1990... Loss: 1.0409... Val Loss: 1.0076\n",
      "Epoch: 9/20... Step: 2000... Loss: 1.0554... Val Loss: 1.0077\n",
      "Epoch: 9/20... Step: 2010... Loss: 1.0602... Val Loss: 1.0076\n",
      "Epoch: 10/20... Step: 2020... Loss: 1.0299... Val Loss: 1.0080\n",
      "Epoch: 10/20... Step: 2030... Loss: 1.0565... Val Loss: 1.0069\n",
      "Epoch: 10/20... Step: 2040... Loss: 1.0204... Val Loss: 1.0084\n",
      "Epoch: 10/20... Step: 2050... Loss: 1.0584... Val Loss: 1.0074\n",
      "Epoch: 10/20... Step: 2060... Loss: 1.0246... Val Loss: 1.0048\n",
      "Epoch: 10/20... Step: 2070... Loss: 1.0499... Val Loss: 1.0055\n",
      "Epoch: 10/20... Step: 2080... Loss: 1.0258... Val Loss: 1.0050\n",
      "Epoch: 10/20... Step: 2090... Loss: 1.0373... Val Loss: 1.0035\n",
      "Epoch: 10/20... Step: 2100... Loss: 1.0488... Val Loss: 1.0046\n",
      "Epoch: 10/20... Step: 2110... Loss: 1.0401... Val Loss: 1.0024\n",
      "Epoch: 10/20... Step: 2120... Loss: 1.0581... Val Loss: 1.0032\n",
      "Epoch: 10/20... Step: 2130... Loss: 1.0477... Val Loss: 1.0022\n",
      "Epoch: 10/20... Step: 2140... Loss: 1.0374... Val Loss: 1.0021\n",
      "Epoch: 10/20... Step: 2150... Loss: 1.0391... Val Loss: 1.0010\n",
      "Epoch: 10/20... Step: 2160... Loss: 1.0434... Val Loss: 0.9989\n",
      "Epoch: 10/20... Step: 2170... Loss: 1.0415... Val Loss: 0.9988\n",
      "Epoch: 10/20... Step: 2180... Loss: 1.0177... Val Loss: 0.9994\n",
      "Epoch: 10/20... Step: 2190... Loss: 1.0171... Val Loss: 0.9996\n",
      "Epoch: 10/20... Step: 2200... Loss: 1.0401... Val Loss: 0.9973\n",
      "Epoch: 10/20... Step: 2210... Loss: 1.0358... Val Loss: 0.9992\n",
      "Epoch: 10/20... Step: 2220... Loss: 1.0327... Val Loss: 0.9951\n",
      "Epoch: 10/20... Step: 2230... Loss: 1.0411... Val Loss: 0.9967\n",
      "Epoch: 10/20... Step: 2240... Loss: 1.1007... Val Loss: 0.9971\n",
      "Epoch: 11/20... Step: 2250... Loss: 1.0084... Val Loss: 0.9972\n",
      "Epoch: 11/20... Step: 2260... Loss: 1.0140... Val Loss: 0.9974\n",
      "Epoch: 11/20... Step: 2270... Loss: 1.0395... Val Loss: 0.9960\n",
      "Epoch: 11/20... Step: 2280... Loss: 1.0406... Val Loss: 0.9954\n",
      "Epoch: 11/20... Step: 2290... Loss: 1.0529... Val Loss: 1.0005\n",
      "Epoch: 11/20... Step: 2300... Loss: 1.0239... Val Loss: 0.9958\n",
      "Epoch: 11/20... Step: 2310... Loss: 1.0614... Val Loss: 0.9968\n",
      "Epoch: 11/20... Step: 2320... Loss: 1.0056... Val Loss: 0.9940\n",
      "Epoch: 11/20... Step: 2330... Loss: 0.9931... Val Loss: 0.9936\n",
      "Epoch: 11/20... Step: 2340... Loss: 1.0162... Val Loss: 0.9935\n",
      "Epoch: 11/20... Step: 2350... Loss: 1.0424... Val Loss: 0.9919\n",
      "Epoch: 11/20... Step: 2360... Loss: 1.0258... Val Loss: 0.9932\n",
      "Epoch: 11/20... Step: 2370... Loss: 1.0095... Val Loss: 0.9945\n",
      "Epoch: 11/20... Step: 2380... Loss: 1.0760... Val Loss: 0.9939\n",
      "Epoch: 11/20... Step: 2390... Loss: 1.0466... Val Loss: 0.9915\n",
      "Epoch: 11/20... Step: 2400... Loss: 0.9954... Val Loss: 0.9904\n",
      "Epoch: 11/20... Step: 2410... Loss: 1.0116... Val Loss: 0.9911\n",
      "Epoch: 11/20... Step: 2420... Loss: 1.0116... Val Loss: 0.9911\n",
      "Epoch: 11/20... Step: 2430... Loss: 1.0447... Val Loss: 0.9891\n",
      "Epoch: 11/20... Step: 2440... Loss: 1.0013... Val Loss: 0.9877\n",
      "Epoch: 11/20... Step: 2450... Loss: 1.0173... Val Loss: 0.9915\n",
      "Epoch: 11/20... Step: 2460... Loss: 1.0253... Val Loss: 0.9911\n",
      "Epoch: 12/20... Step: 2470... Loss: 0.9997... Val Loss: 0.9897\n",
      "Epoch: 12/20... Step: 2480... Loss: 1.0173... Val Loss: 0.9885\n",
      "Epoch: 12/20... Step: 2490... Loss: 0.9731... Val Loss: 0.9892\n",
      "Epoch: 12/20... Step: 2500... Loss: 0.9941... Val Loss: 0.9899\n",
      "Epoch: 12/20... Step: 2510... Loss: 1.0050... Val Loss: 0.9882\n",
      "Epoch: 12/20... Step: 2520... Loss: 0.9951... Val Loss: 0.9879\n",
      "Epoch: 12/20... Step: 2530... Loss: 1.0362... Val Loss: 0.9912\n",
      "Epoch: 12/20... Step: 2540... Loss: 1.0141... Val Loss: 0.9885\n",
      "Epoch: 12/20... Step: 2550... Loss: 1.0197... Val Loss: 0.9888\n",
      "Epoch: 12/20... Step: 2560... Loss: 1.0230... Val Loss: 0.9863\n",
      "Epoch: 12/20... Step: 2570... Loss: 1.0034... Val Loss: 0.9858\n",
      "Epoch: 12/20... Step: 2580... Loss: 1.0176... Val Loss: 0.9853\n",
      "Epoch: 12/20... Step: 2590... Loss: 0.9918... Val Loss: 0.9849\n",
      "Epoch: 12/20... Step: 2600... Loss: 0.9944... Val Loss: 0.9835\n",
      "Epoch: 12/20... Step: 2610... Loss: 1.0090... Val Loss: 0.9841\n",
      "Epoch: 12/20... Step: 2620... Loss: 0.9842... Val Loss: 0.9839\n",
      "Epoch: 12/20... Step: 2630... Loss: 1.0144... Val Loss: 0.9838\n",
      "Epoch: 12/20... Step: 2640... Loss: 1.0127... Val Loss: 0.9838\n",
      "Epoch: 12/20... Step: 2650... Loss: 0.9919... Val Loss: 0.9828\n",
      "Epoch: 12/20... Step: 2660... Loss: 1.0135... Val Loss: 0.9831\n",
      "Epoch: 12/20... Step: 2670... Loss: 1.0236... Val Loss: 0.9797\n",
      "Epoch: 12/20... Step: 2680... Loss: 0.9977... Val Loss: 0.9819\n",
      "Epoch: 13/20... Step: 2690... Loss: 1.0004... Val Loss: 0.9816\n",
      "Epoch: 13/20... Step: 2700... Loss: 0.9965... Val Loss: 0.9811\n",
      "Epoch: 13/20... Step: 2710... Loss: 1.0039... Val Loss: 0.9813\n",
      "Epoch: 13/20... Step: 2720... Loss: 0.9845... Val Loss: 0.9826\n",
      "Epoch: 13/20... Step: 2730... Loss: 1.0077... Val Loss: 0.9790\n",
      "Epoch: 13/20... Step: 2740... Loss: 0.9776... Val Loss: 0.9814\n",
      "Epoch: 13/20... Step: 2750... Loss: 1.0079... Val Loss: 0.9802\n",
      "Epoch: 13/20... Step: 2760... Loss: 1.0165... Val Loss: 0.9804\n",
      "Epoch: 13/20... Step: 2770... Loss: 1.0078... Val Loss: 0.9802\n",
      "Epoch: 13/20... Step: 2780... Loss: 1.0254... Val Loss: 0.9825\n",
      "Epoch: 13/20... Step: 2790... Loss: 0.9967... Val Loss: 0.9801\n",
      "Epoch: 13/20... Step: 2800... Loss: 1.0129... Val Loss: 0.9777\n",
      "Epoch: 13/20... Step: 2810... Loss: 0.9982... Val Loss: 0.9802\n",
      "Epoch: 13/20... Step: 2820... Loss: 0.9756... Val Loss: 0.9798\n",
      "Epoch: 13/20... Step: 2830... Loss: 1.0302... Val Loss: 0.9762\n",
      "Epoch: 13/20... Step: 2840... Loss: 1.0198... Val Loss: 0.9758\n",
      "Epoch: 13/20... Step: 2850... Loss: 0.9804... Val Loss: 0.9784\n",
      "Epoch: 13/20... Step: 2860... Loss: 0.9689... Val Loss: 0.9769\n",
      "Epoch: 13/20... Step: 2870... Loss: 0.9943... Val Loss: 0.9776\n",
      "Epoch: 13/20... Step: 2880... Loss: 0.9789... Val Loss: 0.9786\n",
      "Epoch: 13/20... Step: 2890... Loss: 1.0125... Val Loss: 0.9760\n",
      "Epoch: 13/20... Step: 2900... Loss: 1.0070... Val Loss: 0.9768\n",
      "Epoch: 13/20... Step: 2910... Loss: 1.0229... Val Loss: 0.9778\n",
      "Epoch: 14/20... Step: 2920... Loss: 0.9742... Val Loss: 0.9769\n",
      "Epoch: 14/20... Step: 2930... Loss: 0.9572... Val Loss: 0.9749\n",
      "Epoch: 14/20... Step: 2940... Loss: 1.0029... Val Loss: 0.9745\n",
      "Epoch: 14/20... Step: 2950... Loss: 0.9678... Val Loss: 0.9738\n",
      "Epoch: 14/20... Step: 2960... Loss: 0.9947... Val Loss: 0.9756\n",
      "Epoch: 14/20... Step: 2970... Loss: 1.0049... Val Loss: 0.9732\n",
      "Epoch: 14/20... Step: 2980... Loss: 0.9899... Val Loss: 0.9759\n",
      "Epoch: 14/20... Step: 2990... Loss: 0.9844... Val Loss: 0.9734\n",
      "Epoch: 14/20... Step: 3000... Loss: 0.9698... Val Loss: 0.9739\n",
      "Epoch: 14/20... Step: 3010... Loss: 0.9959... Val Loss: 0.9751\n",
      "Epoch: 14/20... Step: 3020... Loss: 1.0047... Val Loss: 0.9762\n",
      "Epoch: 14/20... Step: 3030... Loss: 0.9876... Val Loss: 0.9745\n",
      "Epoch: 14/20... Step: 3040... Loss: 1.0087... Val Loss: 0.9733\n",
      "Epoch: 14/20... Step: 3050... Loss: 1.0015... Val Loss: 0.9731\n",
      "Epoch: 14/20... Step: 3060... Loss: 1.0060... Val Loss: 0.9758\n",
      "Epoch: 14/20... Step: 3070... Loss: 0.9871... Val Loss: 0.9734\n",
      "Epoch: 14/20... Step: 3080... Loss: 0.9703... Val Loss: 0.9736\n",
      "Epoch: 14/20... Step: 3090... Loss: 0.9832... Val Loss: 0.9722\n",
      "Epoch: 14/20... Step: 3100... Loss: 0.9782... Val Loss: 0.9698\n",
      "Epoch: 14/20... Step: 3110... Loss: 0.9810... Val Loss: 0.9694\n",
      "Epoch: 14/20... Step: 3120... Loss: 0.9852... Val Loss: 0.9704\n",
      "Epoch: 14/20... Step: 3130... Loss: 1.0044... Val Loss: 0.9722\n",
      "Epoch: 15/20... Step: 3140... Loss: 0.9740... Val Loss: 0.9727\n",
      "Epoch: 15/20... Step: 3150... Loss: 0.9931... Val Loss: 0.9709\n",
      "Epoch: 15/20... Step: 3160... Loss: 0.9656... Val Loss: 0.9724\n",
      "Epoch: 15/20... Step: 3170... Loss: 0.9959... Val Loss: 0.9706\n",
      "Epoch: 15/20... Step: 3180... Loss: 0.9687... Val Loss: 0.9706\n",
      "Epoch: 15/20... Step: 3190... Loss: 0.9908... Val Loss: 0.9726\n",
      "Epoch: 15/20... Step: 3200... Loss: 0.9662... Val Loss: 0.9708\n",
      "Epoch: 15/20... Step: 3210... Loss: 0.9750... Val Loss: 0.9700\n",
      "Epoch: 15/20... Step: 3220... Loss: 0.9798... Val Loss: 0.9696\n",
      "Epoch: 15/20... Step: 3230... Loss: 0.9847... Val Loss: 0.9718\n",
      "Epoch: 15/20... Step: 3240... Loss: 1.0010... Val Loss: 0.9712\n",
      "Epoch: 15/20... Step: 3250... Loss: 0.9898... Val Loss: 0.9678\n",
      "Epoch: 15/20... Step: 3260... Loss: 0.9782... Val Loss: 0.9704\n",
      "Epoch: 15/20... Step: 3270... Loss: 0.9773... Val Loss: 0.9684\n",
      "Epoch: 15/20... Step: 3280... Loss: 0.9919... Val Loss: 0.9651\n",
      "Epoch: 15/20... Step: 3290... Loss: 0.9800... Val Loss: 0.9654\n",
      "Epoch: 15/20... Step: 3300... Loss: 0.9505... Val Loss: 0.9670\n",
      "Epoch: 15/20... Step: 3310... Loss: 0.9671... Val Loss: 0.9676\n",
      "Epoch: 15/20... Step: 3320... Loss: 0.9879... Val Loss: 0.9666\n",
      "Epoch: 15/20... Step: 3330... Loss: 0.9760... Val Loss: 0.9671\n",
      "Epoch: 15/20... Step: 3340... Loss: 0.9799... Val Loss: 0.9654\n",
      "Epoch: 15/20... Step: 3350... Loss: 0.9848... Val Loss: 0.9677\n",
      "Epoch: 15/20... Step: 3360... Loss: 1.0495... Val Loss: 0.9690\n",
      "Epoch: 16/20... Step: 3370... Loss: 0.9549... Val Loss: 0.9699\n",
      "Epoch: 16/20... Step: 3380... Loss: 0.9582... Val Loss: 0.9663\n",
      "Epoch: 16/20... Step: 3390... Loss: 0.9682... Val Loss: 0.9662\n",
      "Epoch: 16/20... Step: 3400... Loss: 0.9850... Val Loss: 0.9647\n",
      "Epoch: 16/20... Step: 3410... Loss: 0.9861... Val Loss: 0.9664\n",
      "Epoch: 16/20... Step: 3420... Loss: 0.9709... Val Loss: 0.9648\n",
      "Epoch: 16/20... Step: 3430... Loss: 0.9965... Val Loss: 0.9651\n",
      "Epoch: 16/20... Step: 3440... Loss: 0.9589... Val Loss: 0.9665\n",
      "Epoch: 16/20... Step: 3450... Loss: 0.9480... Val Loss: 0.9658\n",
      "Epoch: 16/20... Step: 3460... Loss: 0.9601... Val Loss: 0.9674\n",
      "Epoch: 16/20... Step: 3470... Loss: 0.9873... Val Loss: 0.9641\n",
      "Epoch: 16/20... Step: 3480... Loss: 0.9794... Val Loss: 0.9652\n",
      "Epoch: 16/20... Step: 3490... Loss: 0.9694... Val Loss: 0.9656\n",
      "Epoch: 16/20... Step: 3500... Loss: 1.0271... Val Loss: 0.9655\n",
      "Epoch: 16/20... Step: 3510... Loss: 1.0014... Val Loss: 0.9673\n",
      "Epoch: 16/20... Step: 3520... Loss: 0.9386... Val Loss: 0.9651\n",
      "Epoch: 16/20... Step: 3530... Loss: 0.9570... Val Loss: 0.9633\n",
      "Epoch: 16/20... Step: 3540... Loss: 0.9615... Val Loss: 0.9631\n",
      "Epoch: 16/20... Step: 3550... Loss: 0.9963... Val Loss: 0.9626\n",
      "Epoch: 16/20... Step: 3560... Loss: 0.9647... Val Loss: 0.9633\n",
      "Epoch: 16/20... Step: 3570... Loss: 0.9717... Val Loss: 0.9633\n",
      "Epoch: 16/20... Step: 3580... Loss: 0.9709... Val Loss: 0.9644\n",
      "Epoch: 17/20... Step: 3590... Loss: 0.9587... Val Loss: 0.9654\n",
      "Epoch: 17/20... Step: 3600... Loss: 0.9667... Val Loss: 0.9626\n",
      "Epoch: 17/20... Step: 3610... Loss: 0.9253... Val Loss: 0.9655\n",
      "Epoch: 17/20... Step: 3620... Loss: 0.9437... Val Loss: 0.9624\n",
      "Epoch: 17/20... Step: 3630... Loss: 0.9446... Val Loss: 0.9630\n",
      "Epoch: 17/20... Step: 3640... Loss: 0.9483... Val Loss: 0.9640\n",
      "Epoch: 17/20... Step: 3650... Loss: 0.9922... Val Loss: 0.9650\n",
      "Epoch: 17/20... Step: 3660... Loss: 0.9629... Val Loss: 0.9629\n",
      "Epoch: 17/20... Step: 3670... Loss: 0.9735... Val Loss: 0.9645\n",
      "Epoch: 17/20... Step: 3680... Loss: 0.9746... Val Loss: 0.9642\n",
      "Epoch: 17/20... Step: 3690... Loss: 0.9524... Val Loss: 0.9630\n",
      "Epoch: 17/20... Step: 3700... Loss: 0.9732... Val Loss: 0.9608\n",
      "Epoch: 17/20... Step: 3710... Loss: 0.9464... Val Loss: 0.9611\n",
      "Epoch: 17/20... Step: 3720... Loss: 0.9549... Val Loss: 0.9589\n",
      "Epoch: 17/20... Step: 3730... Loss: 0.9662... Val Loss: 0.9582\n",
      "Epoch: 17/20... Step: 3740... Loss: 0.9312... Val Loss: 0.9596\n",
      "Epoch: 17/20... Step: 3750... Loss: 0.9616... Val Loss: 0.9620\n",
      "Epoch: 17/20... Step: 3760... Loss: 0.9670... Val Loss: 0.9599\n",
      "Epoch: 17/20... Step: 3770... Loss: 0.9414... Val Loss: 0.9592\n",
      "Epoch: 17/20... Step: 3780... Loss: 0.9682... Val Loss: 0.9602\n",
      "Epoch: 17/20... Step: 3790... Loss: 0.9745... Val Loss: 0.9576\n",
      "Epoch: 17/20... Step: 3800... Loss: 0.9425... Val Loss: 0.9613\n",
      "Epoch: 18/20... Step: 3810... Loss: 0.9569... Val Loss: 0.9614\n",
      "Epoch: 18/20... Step: 3820... Loss: 0.9551... Val Loss: 0.9612\n",
      "Epoch: 18/20... Step: 3830... Loss: 0.9698... Val Loss: 0.9612\n",
      "Epoch: 18/20... Step: 3840... Loss: 0.9390... Val Loss: 0.9585\n",
      "Epoch: 18/20... Step: 3850... Loss: 0.9622... Val Loss: 0.9582\n",
      "Epoch: 18/20... Step: 3860... Loss: 0.9362... Val Loss: 0.9597\n",
      "Epoch: 18/20... Step: 3870... Loss: 0.9692... Val Loss: 0.9590\n",
      "Epoch: 18/20... Step: 3880... Loss: 0.9787... Val Loss: 0.9576\n",
      "Epoch: 18/20... Step: 3890... Loss: 0.9723... Val Loss: 0.9576\n",
      "Epoch: 18/20... Step: 3900... Loss: 0.9746... Val Loss: 0.9564\n",
      "Epoch: 18/20... Step: 3910... Loss: 0.9474... Val Loss: 0.9576\n",
      "Epoch: 18/20... Step: 3920... Loss: 0.9673... Val Loss: 0.9586\n",
      "Epoch: 18/20... Step: 3930... Loss: 0.9656... Val Loss: 0.9591\n",
      "Epoch: 18/20... Step: 3940... Loss: 0.9393... Val Loss: 0.9598\n",
      "Epoch: 18/20... Step: 3950... Loss: 0.9868... Val Loss: 0.9564\n",
      "Epoch: 18/20... Step: 3960... Loss: 0.9664... Val Loss: 0.9590\n",
      "Epoch: 18/20... Step: 3970... Loss: 0.9392... Val Loss: 0.9602\n",
      "Epoch: 18/20... Step: 3980... Loss: 0.9285... Val Loss: 0.9571\n",
      "Epoch: 18/20... Step: 3990... Loss: 0.9529... Val Loss: 0.9572\n",
      "Epoch: 18/20... Step: 4000... Loss: 0.9427... Val Loss: 0.9577\n",
      "Epoch: 18/20... Step: 4010... Loss: 0.9773... Val Loss: 0.9551\n",
      "Epoch: 18/20... Step: 4020... Loss: 0.9622... Val Loss: 0.9564\n",
      "Epoch: 18/20... Step: 4030... Loss: 0.9792... Val Loss: 0.9567\n",
      "Epoch: 19/20... Step: 4040... Loss: 0.9385... Val Loss: 0.9581\n",
      "Epoch: 19/20... Step: 4050... Loss: 0.9236... Val Loss: 0.9568\n",
      "Epoch: 19/20... Step: 4060... Loss: 0.9652... Val Loss: 0.9578\n",
      "Epoch: 19/20... Step: 4070... Loss: 0.9273... Val Loss: 0.9594\n",
      "Epoch: 19/20... Step: 4080... Loss: 0.9603... Val Loss: 0.9584\n",
      "Epoch: 19/20... Step: 4090... Loss: 0.9795... Val Loss: 0.9574\n",
      "Epoch: 19/20... Step: 4100... Loss: 0.9470... Val Loss: 0.9574\n",
      "Epoch: 19/20... Step: 4110... Loss: 0.9452... Val Loss: 0.9561\n",
      "Epoch: 19/20... Step: 4120... Loss: 0.9376... Val Loss: 0.9560\n",
      "Epoch: 19/20... Step: 4130... Loss: 0.9532... Val Loss: 0.9571\n",
      "Epoch: 19/20... Step: 4140... Loss: 0.9648... Val Loss: 0.9600\n",
      "Epoch: 19/20... Step: 4150... Loss: 0.9480... Val Loss: 0.9551\n",
      "Epoch: 19/20... Step: 4160... Loss: 0.9696... Val Loss: 0.9558\n",
      "Epoch: 19/20... Step: 4170... Loss: 0.9629... Val Loss: 0.9557\n",
      "Epoch: 19/20... Step: 4180... Loss: 0.9629... Val Loss: 0.9548\n",
      "Epoch: 19/20... Step: 4190... Loss: 0.9426... Val Loss: 0.9543\n",
      "Epoch: 19/20... Step: 4200... Loss: 0.9352... Val Loss: 0.9573\n",
      "Epoch: 19/20... Step: 4210... Loss: 0.9423... Val Loss: 0.9538\n",
      "Epoch: 19/20... Step: 4220... Loss: 0.9548... Val Loss: 0.9532\n",
      "Epoch: 19/20... Step: 4230... Loss: 0.9426... Val Loss: 0.9527\n",
      "Epoch: 19/20... Step: 4240... Loss: 0.9532... Val Loss: 0.9528\n",
      "Epoch: 19/20... Step: 4250... Loss: 0.9594... Val Loss: 0.9553\n",
      "Epoch: 20/20... Step: 4260... Loss: 0.9277... Val Loss: 0.9559\n",
      "Epoch: 20/20... Step: 4270... Loss: 0.9515... Val Loss: 0.9540\n",
      "Epoch: 20/20... Step: 4280... Loss: 0.9214... Val Loss: 0.9556\n",
      "Epoch: 20/20... Step: 4290... Loss: 0.9633... Val Loss: 0.9555\n",
      "Epoch: 20/20... Step: 4300... Loss: 0.9318... Val Loss: 0.9552\n",
      "Epoch: 20/20... Step: 4310... Loss: 0.9501... Val Loss: 0.9555\n",
      "Epoch: 20/20... Step: 4320... Loss: 0.9276... Val Loss: 0.9552\n",
      "Epoch: 20/20... Step: 4330... Loss: 0.9394... Val Loss: 0.9541\n",
      "Epoch: 20/20... Step: 4340... Loss: 0.9380... Val Loss: 0.9527\n",
      "Epoch: 20/20... Step: 4350... Loss: 0.9480... Val Loss: 0.9517\n",
      "Epoch: 20/20... Step: 4360... Loss: 0.9592... Val Loss: 0.9509\n",
      "Epoch: 20/20... Step: 4370... Loss: 0.9506... Val Loss: 0.9494\n",
      "Epoch: 20/20... Step: 4380... Loss: 0.9419... Val Loss: 0.9524\n",
      "Epoch: 20/20... Step: 4390... Loss: 0.9447... Val Loss: 0.9525\n",
      "Epoch: 20/20... Step: 4400... Loss: 0.9536... Val Loss: 0.9507\n",
      "Epoch: 20/20... Step: 4410... Loss: 0.9452... Val Loss: 0.9528\n",
      "Epoch: 20/20... Step: 4420... Loss: 0.9102... Val Loss: 0.9554\n",
      "Epoch: 20/20... Step: 4430... Loss: 0.9289... Val Loss: 0.9513\n",
      "Epoch: 20/20... Step: 4440... Loss: 0.9448... Val Loss: 0.9514\n",
      "Epoch: 20/20... Step: 4450... Loss: 0.9370... Val Loss: 0.9539\n",
      "Epoch: 20/20... Step: 4460... Loss: 0.9391... Val Loss: 0.9508\n",
      "Epoch: 20/20... Step: 4470... Loss: 0.9454... Val Loss: 0.9500\n",
      "Epoch: 20/20... Step: 4480... Loss: 1.0126... Val Loss: 0.9515\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save a checkpoint of the training we have done to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = './Data/save/rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions, we must take into account that:\n",
    "> Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some  ùêæ\n",
    "  most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moses there is no more any that shall come to perform the land of egypt, that i may see his houses, and they shall say, that which is in the land which i have commanded you from the land which ye shall not dwelt in my hand and when they shall dividing all the workers of iniquity. and it shall be. wherefore the king said to see, the words of the priest shall be a feast toward him. but the lord hath been said, becouse he i any work it with thyer hist all the dreases of all thou, that i may save him as the lord thy god, they shall save thee and i will give thee the son of the lions and the less of the house. and they shall not bring to thee that which they have done and the people of the city, whom the lord shall send them up, and the still within thy selvents, with their flocks they will be wounded and i will say to thee, they sheep and he shall die; if the workers of israel said unto them, teach me i have cast thy father s charge in thy salvation. and when thou shalt say unto them, why way i \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='moses', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
